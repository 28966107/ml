from math import log


class Tensor:
    """
    A simpy system which to calculate gradient automatically without using torch.Each Tensor represents a node in a
    computational graph.The path built by forward propagation decide the path of derivation.The direction of derivation
    is top to down.You need to rewrite more operations and elementary functions if u want to build exclusive autograd
    system belongs to yourself.

    attr:
    ------
        1. val: {float, int}.
            Value of current node.
        2. dependence: {list, None}, Default = [].
            A list for recording nodes which are relied on by current node.It will be updated when building
            computational graph (i.e. forward propagation). The elements in it of form (tensor, grad_fn).The First item
            is a Tensor object and the other is a func for calculating gradient of single layer in computational graph.
            The form of this func is decided by the operation u used in forward propagation.
        3. name: str, Optional, Default = "unspecified"
            Specified name of current node.

    Methods:
    -------
        1. backward: CORE METHOD in this class. A switch that calculate the cumulative gradient from top to bottom.The
        attr 'grad' of any nodes under current node in the computational graph will be updated to be gradient w.r.t
        current node.

        2. zero_grad: Reset all grad of tensors under current node as zero. This method is to be used for solving the
        gradient repeated accumulation repeating because of multiple back propagation.

    Operations:
    ---------
        - __mul__: Rewritten method of left multiplication method for tensor.
        - __rmul__(self, tensor): Rewritten method of right multiplication method for tensor.
        - __add__(self, tensor): Rewritten method of left addition method for tensor.
        - __radd__(self, tensor): Rewritten method of right addition method for tensor.
        - __pow__(self, power): Rewritten method of power method for tensor.

    Example:
    -------
    x = Tensor(2)
    x2 = x * x
    g = x2 * x2
    h = x2 ** 2
    y = g + h
    # Back propagation
    y.backward()

    print("Gradient of y w.r.t x is ", x.grad)

    y.zero_grad()
    g.backward()
    print("Gradient of g w.r.t x is ", x.grad)
    print("val of y is ", y.val)

    z = .99 ** y
    y.zero_grad()
    z.backward()
    print("Gradient of z w.r.t y is ", y.grad)
    """

    def __init__(self, val, dependence=None, name="unspecified") -> None:
        if dependence is None:
            dependence = []
        self.val = val
        self.dependence = dependence
        self.name = name
        # initial grad = 0,it is to storage the Gradient accumulated from top to bottom to the current node.
        self.grad = 0

    def __mul__(self, tensor):
        """
        Left Multiplication: tensor * self
        :param tensor: tensor * self
        :return: New node which constructed by operator __mul__.
        """

        def grad_fn(grad):
            # grad_fn is to calculate the gradient of the node given grad which is calculated in above layer.
            return tensor.val * grad

        def grad_fn2(grad):
            return grad * self.val

        return Tensor(tensor.val * self.val,
                      dependence=[(tensor, grad_fn2), (self, grad_fn)])

    def __rmul__(self, tensor):
        """
        Right Multiplication: self * tensor
        :param tensor:
        :return:
        """

        def grad_fn(grad):
            return grad * tensor.val

        def grad_fn2(grad):
            return self.val * grad

        return Tensor(self.val * tensor.val,
                      dependence=[(self, grad_fn), (tensor, grad_fn2)])

    def __add__(self, tensor):
        """
        Left Add: tensor + self
        :param tensor:Tensor in LHS.
        :return: The tensor generated by left addition operation.
        """

        def grad_fn(grad):
            return grad

        return Tensor(tensor.val + self.val,
                      dependence=[(tensor, grad_fn), (self, grad_fn)])

    def __radd__(self, tensor):
        def grad_fn(grad):
            return grad

        return Tensor(self.val + tensor.val,
                      dependence=[(self, grad_fn), (tensor, grad_fn)])

    def __pow__(self, power):
        """
        power: self ** power
        """

        def grad_fn(grad):
            return grad * power * self.val ** (power - 1)

        return Tensor(self.val ** power,
                      dependence=[(self, grad_fn)])

    def __rpow__(self, base):
        def grad_fn(grad):
            return (base ** self.val) * log(base)

        return Tensor(base ** self.val,
                      dependence=[(self, grad_fn)])

    def backward(self, grad=None):
        # 'grad = None' represents the outset of back propagation.
        if grad is None:
            self.grad = 1
            grad = 1
        else:
            # aggregate the gradients calculated for each path.
            self.grad += grad
        for tensor, grad_fn in self.dependence:
            # calculate the gradient of the current layer
            bp = grad_fn(grad)
            # Calculate the gradient of each node recursively.
            # It is worth mentioning that we aggregate the gradients calculated
            # for each path if tensors in self.dependence are same. For example,
            # we have self.dependence = [(x2, grad_fn), (x2, grad_fn2)] for g because of g = x2 * x2
            # thus all tensors in this loop indicate x2 and self.grad is same object in this recursive depth.
            tensor.backward(bp)

    def zero_grad(self):
        """
        Reset all grad of tensors under current node as zero. This method is to be used for solving the gradient repeated
        accumulation repeating because of multiple back propagation.
        """
        self.grad = 0
        for tensor, grad_fn in self.dependence:
            tensor.zero_grad()


if __name__ == "__main__":
    # Forward propagation to identify dependencies of each node.
    x = Tensor(2)
    x2 = x * x
    g = x2 * x2
    h = x2 ** 2
    y = g + h
    # Back propagation
    y.backward()

    print("Gradient of y w.r.t x is ", x.grad)

    y.zero_grad()
    g.backward()
    print("Gradient of g w.r.t x is ", x.grad)
    print("val of y is ", y.val)

    z = .99 ** y
    y.zero_grad()
    z.backward()
    print("Gradient of z w.r.t y is ", y.grad)
